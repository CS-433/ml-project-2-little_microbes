{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9af15e93-3c92-4bfc-a78b-bbafa1c93d2d",
   "metadata": {},
   "source": [
    "# Model generation and training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ccd261-bce6-4075-9fd5-1800edcd458a",
   "metadata": {},
   "source": [
    "This code aims to create the VAE model with the hyperparameters we chose before. It trains it on the entire dataset and saves it into h5 files. Once this model is generated, the encoder, generator and VAE can be imported and used in other files.\n",
    "\n",
    "The hyperparameters chosen are :\n",
    "- batchsize = 1\n",
    "- intermediate dimension = 64\n",
    "- latent dimension = 2\n",
    "- number of epochs = 120\n",
    "\n",
    "This code was used to generate and train the final model based on the real data, that is saved as ..._final.h5 (can be found in src/ml/samplers/weights_files) and used in the final project to generate the synthetic data.\n",
    "\n",
    "This code can be run again, but will use the fake data for confidentiality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a62929-be86-496a-8b72-db00844f5932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importations\n",
    "from keras import backend as K\n",
    "from keras import losses\n",
    "from keras.layers import Input, LSTM, Dense, Lambda, TimeDistributed\n",
    "from keras.models import Model\n",
    "import pickle\n",
    "from src.ml.samplers.final_utils import *\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "disable_eager_execution()\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91851992-81d5-43f8-aeea-8023c05ae306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 254\n",
      "Number of unique input click types: 22\n",
      "Max number of clicks for one student: 819\n"
     ]
    }
   ],
   "source": [
    "initial_data, max_nb_clicks, num_encoder_clicks, input_clicks, inverse_clicks_dict, click_dict, input_data, decoder_input_data = get_text_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "867abce5-3db5-4f9d-b363-cdf41cde6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data splitting\n",
    "one_hot_train = input_data[:220,:] \n",
    "decoder_input_train = decoder_input_data[:220,:]\n",
    "one_hot_validation = input_data[220:,:] \n",
    "decoder_input_validation = decoder_input_data[220:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ff7f1ed-445a-4b63-ac2c-3f9a8f71e23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)        [(None, None, 23)]           0         []                            \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               (None, 64)                   22528     ['input_6[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 2)                    130       ['lstm_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 2)                    130       ['lstm_2[0][0]']              \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)           (None, 2)                    0         ['dense_4[0][0]',             \n",
      "                                                                     'dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)        [(None, None, 23)]           0         []                            \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 64)                   192       ['lambda_1[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               [(None, None, 64),           22528     ['input_7[0][0]',             \n",
      "                              (None, 64),                            'dense_6[0][0]',             \n",
      "                              (None, 64)]                            'dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDi  (None, None, 23)             1495      ['lstm_3[0][0]']              \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 47003 (183.61 KB)\n",
      "Trainable params: 47003 (183.61 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Train on 254 samples, validate on 34 samples\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 13:34:23.838730: W tensorflow/c/c_api.cc:305] Operation '{name:'training_2/Adam/beta_2/Assign' id:3099 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/beta_2/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/beta_2, training_2/Adam/beta_2/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - ETA: 0s - loss: 0.2903"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valentinedelevaux/anaconda3/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2023-12-15 13:34:47.199150: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_1/mul' id:2761 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/time_distributed_1_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 24s 94ms/sample - loss: 0.2903 - val_loss: 0.2017\n",
      "Epoch 2/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.2112 - val_loss: 0.1701\n",
      "Epoch 3/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1869 - val_loss: 0.1542\n",
      "Epoch 4/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1744 - val_loss: 0.1446\n",
      "Epoch 5/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1636 - val_loss: 0.1388\n",
      "Epoch 6/120\n",
      "254/254 [==============================] - 25s 97ms/sample - loss: 0.1569 - val_loss: 0.1315\n",
      "Epoch 7/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1512 - val_loss: 0.1275\n",
      "Epoch 8/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1471 - val_loss: 0.1245\n",
      "Epoch 9/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1445 - val_loss: 0.1222\n",
      "Epoch 10/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1420 - val_loss: 0.1214\n",
      "Epoch 11/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1416 - val_loss: 0.1202\n",
      "Epoch 12/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1393 - val_loss: 0.1179\n",
      "Epoch 13/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1383 - val_loss: 0.1175\n",
      "Epoch 14/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1377 - val_loss: 0.1170\n",
      "Epoch 15/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1361 - val_loss: 0.1167\n",
      "Epoch 16/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1357 - val_loss: 0.1146\n",
      "Epoch 17/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1352 - val_loss: 0.1144\n",
      "Epoch 18/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1339 - val_loss: 0.1142\n",
      "Epoch 19/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1341 - val_loss: 0.1143\n",
      "Epoch 20/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1345 - val_loss: 0.1125\n",
      "Epoch 21/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1320 - val_loss: 0.1126\n",
      "Epoch 22/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1322 - val_loss: 0.1127\n",
      "Epoch 23/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1316 - val_loss: 0.1118\n",
      "Epoch 24/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1313 - val_loss: 0.1113\n",
      "Epoch 25/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1305 - val_loss: 0.1115\n",
      "Epoch 26/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1301 - val_loss: 0.1107\n",
      "Epoch 27/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1297 - val_loss: 0.1105\n",
      "Epoch 28/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1291 - val_loss: 0.1103\n",
      "Epoch 29/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1286 - val_loss: 0.1096\n",
      "Epoch 30/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1291 - val_loss: 0.1086\n",
      "Epoch 31/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1288 - val_loss: 0.1088\n",
      "Epoch 32/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1276 - val_loss: 0.1079\n",
      "Epoch 33/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1274 - val_loss: 0.1080\n",
      "Epoch 34/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1295 - val_loss: 0.1089\n",
      "Epoch 35/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1271 - val_loss: 0.1071\n",
      "Epoch 36/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1271 - val_loss: 0.1071\n",
      "Epoch 37/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1260 - val_loss: 0.1064\n",
      "Epoch 38/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1256 - val_loss: 0.1059\n",
      "Epoch 39/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1251 - val_loss: 0.1064\n",
      "Epoch 40/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1252 - val_loss: 0.1060\n",
      "Epoch 41/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1245 - val_loss: 0.1054\n",
      "Epoch 42/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1243 - val_loss: 0.1045\n",
      "Epoch 43/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1240 - val_loss: 0.1051\n",
      "Epoch 44/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1238 - val_loss: 0.1046\n",
      "Epoch 45/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1234 - val_loss: 0.1043\n",
      "Epoch 46/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1228 - val_loss: 0.1034\n",
      "Epoch 47/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1228 - val_loss: 0.1036\n",
      "Epoch 48/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1221 - val_loss: 0.1029\n",
      "Epoch 49/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1214 - val_loss: 0.1025\n",
      "Epoch 50/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1213 - val_loss: 0.1016\n",
      "Epoch 51/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1209 - val_loss: 0.1016\n",
      "Epoch 52/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1202 - val_loss: 0.1020\n",
      "Epoch 53/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1201 - val_loss: 0.1012\n",
      "Epoch 54/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1198 - val_loss: 0.1008\n",
      "Epoch 55/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1192 - val_loss: 0.1001\n",
      "Epoch 56/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1184 - val_loss: 0.1002\n",
      "Epoch 57/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1183 - val_loss: 0.0999\n",
      "Epoch 58/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1175 - val_loss: 0.0992\n",
      "Epoch 59/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1171 - val_loss: 0.0985\n",
      "Epoch 60/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1168 - val_loss: 0.0978\n",
      "Epoch 61/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1163 - val_loss: 0.0986\n",
      "Epoch 62/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1158 - val_loss: 0.0975\n",
      "Epoch 63/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1155 - val_loss: 0.0969\n",
      "Epoch 64/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1146 - val_loss: 0.0969\n",
      "Epoch 65/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1144 - val_loss: 0.0953\n",
      "Epoch 66/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1135 - val_loss: 0.0958\n",
      "Epoch 67/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1133 - val_loss: 0.0952\n",
      "Epoch 68/120\n",
      "254/254 [==============================] - 25s 97ms/sample - loss: 0.1132 - val_loss: 0.0960\n",
      "Epoch 69/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1127 - val_loss: 0.0942\n",
      "Epoch 70/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1115 - val_loss: 0.0935\n",
      "Epoch 71/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1116 - val_loss: 0.0938\n",
      "Epoch 72/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1105 - val_loss: 0.0929\n",
      "Epoch 73/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1100 - val_loss: 0.0924\n",
      "Epoch 74/120\n",
      "254/254 [==============================] - 25s 97ms/sample - loss: 0.1097 - val_loss: 0.0922\n",
      "Epoch 75/120\n",
      "254/254 [==============================] - 25s 99ms/sample - loss: 0.1092 - val_loss: 0.0918\n",
      "Epoch 76/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1087 - val_loss: 0.0907\n",
      "Epoch 77/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1079 - val_loss: 0.0903\n",
      "Epoch 78/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1074 - val_loss: 0.0906\n",
      "Epoch 79/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1069 - val_loss: 0.0895\n",
      "Epoch 80/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1064 - val_loss: 0.0897\n",
      "Epoch 81/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1061 - val_loss: 0.0894\n",
      "Epoch 82/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1055 - val_loss: 0.0890\n",
      "Epoch 83/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1053 - val_loss: 0.0882\n",
      "Epoch 84/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1049 - val_loss: 0.0877\n",
      "Epoch 85/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1042 - val_loss: 0.0877\n",
      "Epoch 86/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1038 - val_loss: 0.0863\n",
      "Epoch 87/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1026 - val_loss: 0.0869\n",
      "Epoch 88/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1030 - val_loss: 0.0861\n",
      "Epoch 89/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1023 - val_loss: 0.0854\n",
      "Epoch 90/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.1019 - val_loss: 0.0854\n",
      "Epoch 91/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1012 - val_loss: 0.0855\n",
      "Epoch 92/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1009 - val_loss: 0.0851\n",
      "Epoch 93/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.1006 - val_loss: 0.0839\n",
      "Epoch 94/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.0991 - val_loss: 0.0825\n",
      "Epoch 95/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.0994 - val_loss: 0.0839\n",
      "Epoch 96/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.0990 - val_loss: 0.0842\n",
      "Epoch 97/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.0994 - val_loss: 0.0833\n",
      "Epoch 98/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.0983 - val_loss: 0.0832\n",
      "Epoch 99/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.0973 - val_loss: 0.0802\n",
      "Epoch 100/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.0970 - val_loss: 0.0836\n",
      "Epoch 101/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.0972 - val_loss: 0.0827\n",
      "Epoch 102/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.0961 - val_loss: 0.0804\n",
      "Epoch 103/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.0962 - val_loss: 0.0814\n",
      "Epoch 104/120\n",
      "254/254 [==============================] - 24s 95ms/sample - loss: 0.0958 - val_loss: 0.0813\n",
      "Epoch 105/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.0949 - val_loss: 0.0791\n",
      "Epoch 106/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.0943 - val_loss: 0.0791\n",
      "Epoch 107/120\n",
      "254/254 [==============================] - 25s 97ms/sample - loss: 0.0950 - val_loss: 0.0789\n",
      "Epoch 108/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.0935 - val_loss: 0.0769\n",
      "Epoch 109/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.0926 - val_loss: 0.0778\n",
      "Epoch 110/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.0927 - val_loss: 0.0781\n",
      "Epoch 111/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.0938 - val_loss: 0.0785\n",
      "Epoch 112/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.0925 - val_loss: 0.0767\n",
      "Epoch 113/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.0922 - val_loss: 0.0756\n",
      "Epoch 114/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.0917 - val_loss: 0.0766\n",
      "Epoch 115/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.0916 - val_loss: 0.0764\n",
      "Epoch 116/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.0911 - val_loss: 0.0759\n",
      "Epoch 117/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.0904 - val_loss: 0.0754\n",
      "Epoch 118/120\n",
      "254/254 [==============================] - 25s 97ms/sample - loss: 0.0901 - val_loss: 0.0752\n",
      "Epoch 119/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.0902 - val_loss: 0.0733\n",
      "Epoch 120/120\n",
      "254/254 [==============================] - 24s 96ms/sample - loss: 0.0884 - val_loss: 0.0745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x15b976c10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parameters\n",
    "input_dim=len(input_data[0][0]) #corresponds to the length of the one-hot vectors\n",
    "batch_size = 1\n",
    "intermediate_dim = 64\n",
    "latent_dim = 2\n",
    "num_epochs = 120\n",
    "\n",
    "#Model creation\n",
    "vae, encoder, generator, stepper = create_lstm_vae(input_dim, batch_size, intermediate_dim, latent_dim)\n",
    "\n",
    "#Model training\n",
    "vae.fit(x=[input_data, decoder_input_data], y=input_data,batch_size=batch_size, epochs=num_epochs, verbose=1, validation_data = ([one_hot_validation,decoder_input_validation], one_hot_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea65d5ea-a482-475e-b81d-dbf6fba90da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model saving\n",
    "vae.save('model_vae_.h5')\n",
    "generator.save_weights('generator_weights_.h5')\n",
    "encoder.save_weights('encoder_weights_.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
